{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J1lmK2Viqey"
      },
      "source": [
        "# Explainability-aided text-guided image editing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An adaptation of StyleCLIP (https://github.com/orpatashnik/StyleCLIP) optimization notebook, that uses explainability-guided loss"
      ],
      "metadata": {
        "id": "YhgNuoFQREoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "2PejTNuzTCTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup\n",
        "\n",
        "!git clone https://github.com/apple/ml-no-token-left-behind.git\n",
        "import os\n",
        "os.chdir(f'ml-no-token-left-behind')\n",
        "\n",
        "!pip install ftfy regex tqdm captum torchvision opencv-python matplotlib\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# downloads StyleGAN's weights and facial recognition network weights\n",
        "ids = ['1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT', '1N0MZSqPRJpLfP4mFQCS14ikrVSe8vQlL']\n",
        "for file_id in ids:\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.FetchMetadata(fetch_all=True)\n",
        "  downloaded.GetContentFile(downloaded.metadata['title'])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "b89LM9mqTLTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ooxLY3NJPn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Explainabilitiy utils\n",
        "import torch\n",
        "import external.TransformerMMExplainability\n",
        "import external.TransformerMMExplainability.CLIP.clip as clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from captum.attr import visualization\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# create heatmap from mask on image\n",
        "def show_cam_on_image(img, mask):\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "    return cam\n",
        "\n",
        "def get_desired_tokens_from_words(desired_words, text):\n",
        "    if desired_words is None:\n",
        "        num_of_tokens = len(_tokenizer.encode(text))\n",
        "        desired_tokens = torch.full((num_of_tokens,), 1 / num_of_tokens)\n",
        "    \n",
        "    else:\n",
        "\n",
        "        target_words = text.split(\" \")\n",
        "        desired_words = np.array(desired_words.split(' ')).astype(float)\n",
        "        desired_tokens = torch.zeros((len(_tokenizer.encode(text))))\n",
        "\n",
        "        token_id = 0\n",
        "        for word_idx, word in enumerate(target_words):\n",
        "            num_of_tokens = len(_tokenizer.encode(word))\n",
        "\n",
        "            for t in range(num_of_tokens):\n",
        "                desired_tokens[token_id] = desired_words[word_idx]\n",
        "                token_id = token_id + 1\n",
        "                \n",
        "        if desired_tokens.min() != desired_tokens.max():\n",
        "            desired_tokens /= desired_tokens.max()\n",
        "        else:\n",
        "            desired_tokens = desired_tokens / len(desired_tokens)\n",
        "    \n",
        "    return desired_tokens\n",
        "\n",
        "def interpret(image, text, model, device, desired_words, index=None, softmax_temp=10.):\n",
        "    model.zero_grad()\n",
        "    desired_words = get_desired_tokens_from_words(desired_words, text)\n",
        "    \n",
        "    text = clip.tokenize([text]).to(device)\n",
        "    CLS_idx = text.argmax(dim=-1)\n",
        "    \n",
        "\n",
        "    with torch.enable_grad():\n",
        "        image = image.detach().clone().requires_grad_() # TODO: what should require grad? the model? I need the image\n",
        "        logits_per_image, logits_per_text = model(image, text)\n",
        "        probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
        "        if index is None:\n",
        "            index = np.argmax(logits_per_image.cpu().data.numpy(), axis=-1)\n",
        "        one_hot = np.zeros((1, logits_per_image.size()[-1]), dtype=np.float32)\n",
        "        one_hot[0, index] = 1\n",
        "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "        one_hot =  torch.sum(one_hot.to(logits_per_image.device) * logits_per_image)\n",
        "\n",
        "        image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
        "        num_tokens = image_attn_blocks[0].attn.attn_output_weights.shape[-1]\n",
        "        R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn.attn_output_weights.dtype).to(logits_per_image.device)\n",
        "        for blk_idx, blk in enumerate(image_attn_blocks):\n",
        "            grad = torch.autograd.grad(one_hot, [blk.attn.attn_output_weights], retain_graph=True)[0].detach()\n",
        "            cam = blk.attn.attn_output_weights.detach()\n",
        "            cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "            grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "            cam = grad * cam\n",
        "            cam = cam.clamp(min=0).mean(dim=0)\n",
        "            R = R + torch.matmul(cam, R)\n",
        "        R[0, 0] = 0\n",
        "        image_relevance = R[0, 1:]\n",
        "\n",
        "        text_attn_blocks = list(dict(model.transformer.resblocks.named_children()).values())\n",
        "        num_tokens = text_attn_blocks[0].attn.attn_output_weights.shape[-1]\n",
        "        R_text = torch.eye(num_tokens, num_tokens, dtype=text_attn_blocks[0].attn.attn_output_weights.dtype).to(logits_per_image.device)\n",
        "        for blk_idx, blk in enumerate(text_attn_blocks):\n",
        "            grad = torch.autograd.grad(one_hot, [blk.attn.attn_output_weights], retain_graph=True)[0].detach()\n",
        "            cam = blk.attn.attn_output_weights.detach()\n",
        "            cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "            grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "            cam = grad * cam\n",
        "            cam = cam.clamp(min=0).mean(dim=0)\n",
        "            R_text = R_text + torch.matmul(cam, R_text)\n",
        "        text_relevance = R_text[CLS_idx, 1:CLS_idx]\n",
        "        text_relevance = text_relevance / text_relevance.sum()\n",
        "        text_relevance = text_relevance / text_relevance.max()\n",
        "        target_word_expl_score = (text_relevance * desired_words.to(logits_per_image.device))\n",
        "\n",
        "    image_relevance = image_relevance.reshape(1, 1, 7, 7)\n",
        "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
        "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
        "    image_relevance = image_relevance / image_relevance.sum()\n",
        "    image_relevance = image_relevance / image_relevance.max()\n",
        "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    vis = show_cam_on_image(image, image_relevance)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    plt.imshow(vis)\n",
        "    model.zero_grad()\n",
        "    return text_relevance.detach(), logits_per_image.detach(), target_word_expl_score.detach()\n",
        "\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "\n",
        "from external.TransformerMMExplainability.CLIP.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "_tokenizer = _Tokenizer()\n",
        "\n",
        "def show_heatmap_on_text(text, text_encoding, R_text):\n",
        "  text_scores = R_text.flatten()\n",
        "#   print(text_scores)\n",
        "  text_tokens=_tokenizer.encode(text)\n",
        "  text_tokens_decoded=[_tokenizer.decode([a]) for a in text_tokens]\n",
        "  vis_data_records = [visualization.VisualizationDataRecord(text_scores,0,0,0,0,0,text_tokens_decoded,1)]\n",
        "  visualization.visualize_text(vis_data_records)\n",
        "  return text_scores\n",
        "\n",
        "def visualize_explainability(image, text, desired_words, model, width):\n",
        "    source_image = torch.nn.AvgPool2d(kernel_size=width // 32)(\n",
        "        torch.nn.Upsample(scale_factor=7)(image.to(clip_device)))\n",
        "\n",
        "    R_text, _, _ = interpret(image=source_image, \n",
        "                             text=text,\n",
        "                             model=model,\n",
        "                             desired_words=desired_words,\n",
        "                             device=clip_device, \n",
        "                             index=0)\n",
        "        \n",
        "    show_heatmap_on_text(text, clip.tokenize([text]), R_text)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def show_heatmap_on_text(text, text_encoding, R_text, softmax_temp=2.):\n",
        "  CLS_idx = text_encoding.argmax(dim=-1)\n",
        "  R_text = R_text[CLS_idx, 1:CLS_idx]\n",
        "  text_scores = R_text / R_text.sum()\n",
        "  text_scores = text_scores.flatten()\n",
        "  text_tokens=_tokenizer.encode(text)\n",
        "  text_tokens_decoded=[_tokenizer.decode([a]) for a in text_tokens]\n",
        "  vis_data_records = [visualization.VisualizationDataRecord(text_scores,0,0,0,0,0,text_tokens_decoded,1)]\n",
        "  visualization.visualize_text(vis_data_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run configuration\n",
        "Please enter your manipulation text in th description, desired words should be 1 for words that are part of the semnatic change in the image"
      ],
      "metadata": {
        "id": "1i7B2qGSSdNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTAVTULlq87j",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "description = 'A person with purple hair' #@param {type:\"string\"}\n",
        "\n",
        "desired_words = \"0 0 0 1 1\"#@param {type:\"string\"}\n",
        "\n",
        "optimization_steps = 200 #@param {type:\"number\"}\n",
        "\n",
        "expl_lambda = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "l2_lambda = 0.008 #@param {type:\"number\"}\n",
        "\n",
        "id_lambda = 0.005 #@param {type:\"number\"}\n",
        "\n",
        "stylespace = False #@param {type:\"boolean\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cuXs6QD8mKjI"
      },
      "outputs": [],
      "source": [
        "use_seed = True #@param {type:\"boolean\"}\n",
        "\n",
        "seed =  0#@param {type: \"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "DI_UONvJTTwS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcBz_eEomF7Q"
      },
      "outputs": [],
      "source": [
        "#@title Additional Arguments\n",
        "args = {\n",
        "    \"description\": description,\n",
        "    \"desired_words\": desired_words,\n",
        "    \"expl_lambda\": expl_lambda,\n",
        "    \"ckpt\": \"stylegan2-ffhq-config-f.pt\",\n",
        "    \"stylegan_size\": 1024,\n",
        "    \"lr_rampup\": 0.05,\n",
        "    \"lr\": 0.1,\n",
        "    \"step\": optimization_steps,\n",
        "    \"mode\": \"edit\",\n",
        "    \"l2_lambda\": l2_lambda,\n",
        "    \"id_lambda\": id_lambda,\n",
        "    'work_in_stylespace': stylespace,\n",
        "    \"latent_path\": None,\n",
        "    \"truncation\": 0.7,\n",
        "    \"save_intermediate_image_every\": 20,\n",
        "    \"results_dir\": \"results\",\n",
        "    \"ir_se50_weights\": \"model_ir_se50.pth\",\n",
        "    \"loss_type\": \"EGCLIPLoss\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT9JRl8hnT1l"
      },
      "outputs": [],
      "source": [
        "if use_seed:\n",
        "  import torch\n",
        "  torch.manual_seed(seed)\n",
        "from external.StyleCLIP.optimization.run_optimization import main\n",
        "from argparse import Namespace\n",
        "result = main(Namespace(**args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h15xcbHwnW0U"
      },
      "outputs": [],
      "source": [
        "#@title Visualize Result\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms import ToPILImage\n",
        "result_image = ToPILImage()(make_grid(result[0].detach().cpu(), normalize=True, scale_each=True, range=(-1, 1), padding=0))\n",
        "h, w = result_image.size\n",
        "result_image.resize((h // 2, w // 2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2PejTNuzTCTJ",
        "DI_UONvJTTwS"
      ],
      "name": "Explainability_aided_image_manipulation.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
