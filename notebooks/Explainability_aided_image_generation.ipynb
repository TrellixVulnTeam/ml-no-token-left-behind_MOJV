{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_generation-2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "go9Vz098Uk5g"
      ],
      "machine_shape": "hm",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "# Explainability-aided Image generation\n",
        "\n",
        "Built upon FuseDream by Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su and Qiang Liu (https://github.com/gnobitab/FuseDream). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "go9Vz098Uk5g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkUfzT60ZZ9q"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/apple/ml-no-token-left-behind.git\n",
        "import os\n",
        "os.chdir(f'ml-no-token-left-behind')\n",
        "!pip install ftfy regex tqdm numpy scipy h5py lpips==0.1.4 flair sacremoses\n",
        "!pip install gdown captum\n",
        "!gdown 'https://drive.google.com/uc?id=1YqbbmUijKI85WZjTdRD2mMp4CDaDUWgC'\n",
        "!gdown 'https://drive.google.com/uc?id=1dr196QReWq0UWF7pQSZcbCiw0ksbexpk'\n",
        "!mkdir external/FuseDream/BigGAN_utils/weights/\n",
        "!cp biggan-256.pth external/FuseDream/BigGAN_utils/weights/\n",
        "!cp biggan-512.pth external/FuseDream/BigGAN_utils/weights/"
      ],
      "metadata": {
        "id": "jxwnDUrsdJkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AXgGDr_K3NV"
      },
      "source": [
        "import sys\n",
        "sys.path.append('./external/FuseDream')\n",
        "sys.path.append('./external/FuseDream/BigGAN_utils')\n",
        "sys.path.append(\"./external/TransformerMMExplainability\")\n",
        "import external.TransformerMMExplainability.CLIP.clip as clip\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import external.FuseDream.BigGAN_utils.utils as utils\n",
        "import torch.nn.functional as F\n",
        "from external.FuseDream.DiffAugment_pytorch import DiffAugment\n",
        "import numpy as np\n",
        "from external.FuseDream.fusedream_utils import FuseDreamBaseGenerator, get_G, save_image, interpret, show_heatmap_on_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1paKfbekkmz"
      },
      "source": [
        "## Setting up parameters\n",
        "1. SENTENCE: The query text for generating the image. Note: we find that putting a period '.' at the end of the sentence can boost the quality of the generated images, e.g., 'A photo of a blue dog.' generates better images than 'A photo of a blue dog'.\n",
        "2. INIT_ITERS: Controls the number of images used for initialization (M in the paper, and M = INIT_ITERS*10). Use the default number 1000 should work well.\n",
        "3. OPT_ITERS: Controls the number of iterations for optimizing the latent variables. Use the default number 1000 should work well.\n",
        "4. NUM_BASIS: Controls the number of basis images used in optimization (k in the paper). Choose from 5, 10, 15 should work well.\n",
        "5. MODEL: Currently please choose from 'biggan-256' and 'biggan-512'.\n",
        "6. SEED: Random seed. Choose an arbitrary integer you like.\n",
        "7. LAMBDA_EXPL - the weighting of the explainability-based loss\n",
        "8. NEGLECT_THRESHOLD - the threshold of relevance under which a word is considered neglected in the generated image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utIHfdoejnJg",
        "cellView": "form"
      },
      "source": [
        "#@title Parameters\n",
        "SENTENCE = \"A photo of a strawberry muffin\" #@param {type:\"string\"}\n",
        "INIT_ITERS =  1000#@param {type:\"number\"}\n",
        "OPT_ITERS = 1000#@param {type:\"number\"}\n",
        "NUM_BASIS = 10#@param {type:\"number\"}\n",
        "MODEL = \"biggan-512\" #@param [\"biggan-256\",\"biggan-512\"]\n",
        "SEED = 0#@param {type:\"number\"}\n",
        "LAMBDA_EXPL = 0.1#@param {type:\"number\"}\n",
        "NEGLECT_THRESHOLD = 0.7#@param {type:\"number\"}\n",
        "\n",
        "import sys\n",
        "sys.argv = [''] ### workaround to deal with the argparse in Jupyter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "45IyuKgSUgVS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMSuW2EQWsd",
        "cellView": "form"
      },
      "source": [
        "#@title Original FuseDream Generation\n",
        "from external.TransformerMMExplainability.CLIP.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "_tokenizer = _Tokenizer()\n",
        "\n",
        "utils.seed_rng(SEED) \n",
        "\n",
        "sentence = SENTENCE\n",
        "\n",
        "print('Generating:', sentence)\n",
        "if MODEL == \"biggan-256\":\n",
        "    G, config = get_G(256) \n",
        "elif MODEL == \"biggan-512\":\n",
        "    G, config = get_G(512) \n",
        "else:\n",
        "    raise Exception('Model not supported')\n",
        "generator = FuseDreamBaseGenerator(G, config, 10)\n",
        "z_cllt, y_cllt = generator.generate_basis(sentence,\n",
        "                                          init_iters=INIT_ITERS,\n",
        "                                          num_basis=NUM_BASIS,\n",
        "                                          expl_lambda=0)\n",
        "\n",
        "z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n",
        "y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n",
        "img, z, y = generator.optimize_clip_score(z_cllt,\n",
        "                                          y_cllt,\n",
        "                                          sentence, \n",
        "                                          latent_noise=False, \n",
        "                                          augment=True, \n",
        "                                          opt_iters=OPT_ITERS, \n",
        "                                          optimize_y=True,\n",
        "                                          expl_lambda=0)\n",
        "score = generator.measureAugCLIP(z, y, sentence, augment=True, num_samples=20)\n",
        "print('AugCLIP score for original FuseDream result:', score)\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "print(\"resulting image\")\n",
        "display.display(torchvision.transforms.functional.to_pil_image(torchvision.utils.make_grid(img.detach().cpu(), nrow=1, normalize=True, scale_each=True, range=(-1, 1), padding=0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check if any object is neglected\n",
        "\n",
        "from flair.models import MultiTagger\n",
        "from flair.data import Sentence\n",
        "tagger = MultiTagger.load(['pos'])\n",
        "\n",
        "normalize = torchvision.transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "img_res = F.interpolate(img, size=224, mode='bilinear')\n",
        "text_relevance = interpret(normalize(img_res), sentence, model=generator.clip_model, device='cuda')\n",
        "text_scores = [str(score) for score in text_relevance[0].detach().cpu().numpy()]\n",
        "\n",
        "text_expl_score = text_relevance\n",
        "desired_tokens = 0\n",
        "pos = {}\n",
        "tag_lst = []\n",
        "desired_tokens = torch.zeros((len(_tokenizer.encode(sentence))))\n",
        "words_expl_scores = []\n",
        "token_id = 0\n",
        "text_tokens_decoded=[_tokenizer.decode([a]) for a in _tokenizer.encode(sentence)]\n",
        "entire_word = ''\n",
        "\n",
        "sentence_obj = Sentence(sentence)\n",
        "tagger.predict(sentence_obj)\n",
        "\n",
        "for label in sentence_obj.get_labels('pos'):\n",
        "    print(label)\n",
        "    \n",
        "    entire_word = entire_word + label.data_point.text\n",
        "\n",
        "    # if is part of token\n",
        "    if text_tokens_decoded[token_id] != entire_word.lower() and \\\n",
        "        text_tokens_decoded[token_id] != f'{entire_word} '.lower() and \\\n",
        "        text_tokens_decoded[token_id].startswith(entire_word.lower()):\n",
        "        continue\n",
        "    else:\n",
        "        tag_lst.append({'word': entire_word, 'POS': label.value})\n",
        "\n",
        "        num_of_tokens = len(_tokenizer.encode(entire_word))\n",
        "        for t in range(num_of_tokens):\n",
        "            token_id = token_id + 1\n",
        "        entire_word = ''\n",
        "\n",
        "needs_our_method = False\n",
        "token_id = 0\n",
        "for word_idx, pos_dict in enumerate(tag_lst):\n",
        "    word, pos = pos_dict['word'], pos_dict['POS']\n",
        "\n",
        "    num_of_tokens = len(_tokenizer.encode(word))\n",
        "\n",
        "    expl = 0\n",
        "    beg_token_id = token_id\n",
        "    for t in range(num_of_tokens):\n",
        "        if text_expl_score[0, token_id] > expl:\n",
        "            expl = text_expl_score[0, token_id]\n",
        "        token_id += 1\n",
        "\n",
        "    tag_lst[word_idx]['expl'] = expl\n",
        "    tag_lst[word_idx]['tokens'] = list(range(beg_token_id, token_id))\n",
        "\n",
        "    tag_lst[word_idx]['need_emphasize'] = False\n",
        "    if pos.startswith('NN'):\n",
        "        \n",
        "        if expl < NEGLECT_THRESHOLD:\n",
        "                tag_lst[word_idx]['need_emphasize'] = True\n",
        "                needs_our_method = True\n",
        "    \n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "BWcFGoIvUG4j",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Explainability-aided generation\n",
        "if needs_our_method:\n",
        "    desired_tokens = [word['tokens'] for word in tag_lst if word['need_emphasize']]\n",
        "    print(desired_tokens)\n",
        "\n",
        "    utils.seed_rng(SEED) \n",
        "\n",
        "    sentence = SENTENCE\n",
        "\n",
        "    print('Generating:', sentence)\n",
        "    if MODEL == \"biggan-256\":\n",
        "        G, config = get_G(256) \n",
        "    elif MODEL == \"biggan-512\":\n",
        "        G, config = get_G(512) \n",
        "    else:\n",
        "        raise Exception('Model not supported')\n",
        "    generator = FuseDreamBaseGenerator(G, config, 10) \n",
        "    z_cllt, y_cllt = generator.generate_basis(sentence,\n",
        "                                              init_iters=INIT_ITERS,\n",
        "                                              num_basis=NUM_BASIS,\n",
        "                                              desired_tokens=desired_tokens,\n",
        "                                              expl_lambda=LAMBDA_EXPL)\n",
        "\n",
        "    z_cllt_save = torch.cat(z_cllt).cpu().numpy()\n",
        "    y_cllt_save = torch.cat(y_cllt).cpu().numpy()\n",
        "    img, z, y = generator.optimize_clip_score(z_cllt,\n",
        "                                              y_cllt,\n",
        "                                              sentence, \n",
        "                                              latent_noise=False, \n",
        "                                              augment=True, \n",
        "                                              opt_iters=OPT_ITERS, \n",
        "                                              optimize_y=True,\n",
        "                                              desired_words = desired_tokens,\n",
        "                                              expl_lambda=0)\n",
        "    score = generator.measureAugCLIP(z, y, sentence, augment=True, num_samples=20)\n",
        "    print('AugCLIP score for explainability-aided FuseDream result:', score)\n",
        "\n",
        "    from IPython import display\n",
        "\n",
        "    print(\"resulting image\")\n",
        "    display.display(torchvision.transforms.functional.to_pil_image(torchvision.utils.make_grid(img.detach().cpu(), nrow=1, normalize=True, scale_each=True, range=(-1, 1), padding=0)))\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"No object is neglected, no explainability-assistance is needed\")"
      ],
      "metadata": {
        "id": "EETC4qZ2UvQm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}